{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea6a0a4-c954-4875-8ab5-b39551e3532f",
   "metadata": {},
   "source": [
    "<H2>Pré requisitos para rodar o codigo</H2>\n",
    "\n",
    "Ter o ollama instalado na maquina para usar o modelo Gemma2:2b\n",
    "\n",
    "<ol>\n",
    "    <li>Baixe o instalador do <a href=\"https://ollama.com/download\">site oficial</a> </li>\n",
    "    <li>Execute o arquivo .exe e siga as instruções do assistente de instalação.</li>\n",
    "    <li>Após a instalação, abra o terminal (cmd ou PowerShell) e verifique se Ollama está instalado corretamente com o comando ollama --version.</li>\n",
    "    <li>No terminal (cmd ou PowerShell) de o comando \"ollama pull gemma2:2b\" para baixar o modelo </li>\n",
    "    <li>Asimm que finalizar o downlaod de o comando \"ollama pull nomic-embed-text\" que vamos usar para gerar embeddings</li>\n",
    "</ol>\n",
    "\n",
    "Instalações\n",
    "\n",
    "<p>Comando usando o conda <br>\n",
    "    conda install Langchain-community langachain-experimental PyPDF sentence-transformers chromadb langchain_ollama\n",
    "</p>\n",
    "<p>Comando usando o pip <br>\n",
    "    pip install Langchain-community langachain-experimental PyPDF sentence-transformers chromadb langchain_ollama\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe31932e",
   "metadata": {},
   "source": [
    "<h3>Bibliotecas usadas</H3>\n",
    "<h4>Langchain</H4>\n",
    "<p>Langchain é uma biblioteca de Python que facilita a criação de aplicações de processamento de linguagem natural (NLP) complexas e personalizadas. Ela oferece uma interface intuitiva para conectar diferentes componentes de NLP, como modelos de linguagem, vetores, e fontes de dados. A documentação pode ser encontrada <a href=\"https://python.langchain.com/docs/introduction/\">clicando aqui</a>  </p>\n",
    "<h4>Transformers </H4>\n",
    "<p>Transformers é uma arquitetura de rede neural que revolucionou o campo do NLP. Modelos como GPT-3 e BERT são baseados em transformers e são capazes de realizar tarefas complexas como tradução de idiomas, geração de texto e resposta a perguntas. <a href=\"https://huggingface.co/docs/transformers/index\">clicando aqui</a>  </p>\n",
    "<h4>Chromadb</H4>\n",
    "<p>Chroma é um banco de dados de vetores especialmente projetado para aplicações de IA. Ele armazena e recupera embeddings (representações numéricas de texto) de forma eficiente, o que é fundamental para tarefas como busca semântica e recomendação. <a href=\"https://docs.trychroma.com/\">clicando aqui</a>  </p>\n",
    "<h4>Ollama</H4>\n",
    "<p>Ollama é uma ferramenta que permite rodar modelos de linguagem de grande escala (LLMs) localmente, no próprio computador. Isso significa que podemos utilizar modelos como gemma 2 e o nomic-embed-text sem depender de APIs externas, o que pode reduzir custos e aumentar a privacidade dos dados. A documentação que podemos seguir é a que esta no langchain ja que vamos integrar o modelo a essa biblioteca, a documentação pode ser acessada <a href=\"https://python.langchain.com/api_reference/ollama/index.html\">clicando aqui</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba95f01",
   "metadata": {},
   "source": [
    "\n",
    "<H5>0 - Defininfo o modelo como o gemma2 de 2 bilões de parametros</H5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d04f4f55-ae17-4173-b2f4-3ec1e9dd058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "model = ChatOllama(model='gemma2:2b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d00216-3514-4059-afe0-2a0d889e7cd6",
   "metadata": {},
   "source": [
    "<H5>1 - Extração e tratamento de Dados</H5>\n",
    "<ul>\n",
    "<li>loader.load() - Carrega o texto conforme o caminho passado</li>\n",
    "<li>HuggingFaceEmbeddings() - Gera embeddings</li>\n",
    "<li>SemanticChunker()- dividir textos de forma semantica.\n",
    "    <ul>\n",
    "        <li>breakpoint_threshold_type - decidir onde o texto sera dividido nesse caso foi definido como \"standard_deviation\" significa que o algoritmo ira analisar a similaridade semantica entre as partes do texto e identificar os pontos onde essa   similaridade muda significativamente.</li>\n",
    "    </ul>\n",
    " </li>\n",
    "<li>split_documents() - divide o documento em partes menores</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5dab68-d8db-40f7-8503-33080e505676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Caminho do PDF\")\n",
    "\n",
    "docs_split = loader.load()\n",
    "hf_embeddings = HuggingFaceEmbeddings()\n",
    "text_splitter = SemanticChunker(hf_embeddings, breakpoint_threshold_type=\"standard_deviation\") \n",
    "chunks = text_splitter.split_documents(docs_split)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a01baa-3aec-4cc3-916c-f7d448239719",
   "metadata": {},
   "source": [
    "<H5>2 - Convertendo chunks para embeddings e armarmazenando</H5>\n",
    "<ul>\n",
    "    <li>\n",
    "        Chroma.from_documents() - cria a vector store\n",
    "        <ul>\n",
    "            <li>documents - passa os chunks de texto gerados na celula anterior</li>\n",
    "            <li>embedding - define que os embeddings seraão gerados pelo modelo nomic-embed-text</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>retriever = usado para buscar os documentos mais relevantes dentro do banco de dados, com base em uma query</li>\n",
    "</ul>\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be881693-5c0f-4ad2-8b92-f6d3e806042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OllamaEmbeddings(model='nomic-embed-text')\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a99f1b-d6b8-434b-b091-ce1fc8b8f2bb",
   "metadata": {},
   "source": [
    "<H5>3 - Fazendo a pergunta</H5>\n",
    "<ul>\n",
    "    <li>prompt_templat - Define o formato da pergunta que será feita ao modelo de linguagem.</li>\n",
    "    <li>ChatPromptTemplate - Cria um objeto ChatPromptTemplate a partir do template definido.\n",
    "        <ul>\n",
    "            <li>from_template() - define o template da pergunta que será respondida</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>chain - Define a cadeia de passos que o modelo seguirá para responder à pergunta.\n",
    "        <ul>\n",
    "            <li>retriever - Busca o contexto relevante para a pergunta.</li>\n",
    "            <li>RunnablePassthrough() - Passa a pergunta diretamente para o próximo passo.</li>\n",
    "            <li>prompt - Formata a pergunta final usando o template e o contexto.</li>\n",
    "            <li>model - O modelo de linguagem processa a pergunta formatada e gera a resposta.</li>\n",
    "            <li>StrOutputParser() - Converte a saída do modelo em uma string.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>invoke - Executa a cadeia de passos definida e imprime a resposta gerada pelo modelo.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1ff626-0ff1-4b3f-8710-756a6399684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_templat = \"\"\"Responda à pergunta com base apenas no seguinte contexto:\n",
    "{context}\n",
    "Pergunta: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_templat)\n",
    "chain = ({'context': retriever, 'question': RunnablePassthrough()} |\n",
    "                   prompt | model | StrOutputParser())\n",
    "while True:\n",
    "    question = input(\"Digite a pergunta : \")\n",
    "    print(\"Pensando:\\n\")\n",
    "    print(chain.invoke(question))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
